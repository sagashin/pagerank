{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter web url or enter: https://news.yahoo.co.jp\n",
      "['https://news.yahoo.co.jp']\n",
      "How many pages:10\n",
      "1 https://news.yahoo.co.jp (39882) 55\n",
      "21 https://news.yahoo.co.jp/pickup/6254197 (41229) 56\n",
      "36 https://news.yahoo.co.jp/ranking/comment/rate?ty=t (48877) 57\n",
      "39 https://news.yahoo.co.jp/polls/domestic/30763/vote (25829) 50\n",
      "12 https://news.yahoo.co.jp/hl?c=dom (47808) 67\n",
      "61 https://news.yahoo.co.jp/polls/sports/list (33244) 85\n",
      "37 https://news.yahoo.co.jp/ranking/access?ty=v (44114) 48\n",
      "79 https://news.yahoo.co.jp/pickup/6254198 (40125) 56\n",
      "42 https://news.yahoo.co.jp/polls/domestic/30683/vote (27497) 50\n",
      "14 https://news.yahoo.co.jp/hl?c=bus (52437) 71\n",
      "How many pages:100\n",
      "130 https://news.yahoo.co.jp/pickup/6254155 (40360) 56\n",
      "84 https://news.yahoo.co.jp/ranking/access?ty=z&c=dom (45648) 56\n",
      "83 https://news.yahoo.co.jp/hl?c=dom&p=1 (39133) 61\n",
      "137 https://news.yahoo.co.jp/ranking/access?ty=z&c=bus (47541) 56\n",
      "109 https://news.yahoo.co.jp/polls/sports/28422/result (27382) 49\n",
      "72 https://news.yahoo.co.jp/hl?c=soci (44034) 65\n",
      "53 https://news.yahoo.co.jp/ranking/comment/rate?ty=t&c=c_ent (54441) 57\n",
      "59 https://news.yahoo.co.jp/polls/domestic/list (33216) 85\n",
      "106 https://news.yahoo.co.jp/polls/sports/28922/result (25801) 49\n",
      "94 https://news.yahoo.co.jp/polls/sports/list?page=2 (33330) 87\n",
      "66 https://news.yahoo.co.jp/polls/entertainment/30783/vote (27015) 51\n",
      "185 https://news.yahoo.co.jp/polls/domestic/30143/result (25729) 49\n",
      "207 https://news.yahoo.co.jp/polls/sports/26402/result (29451) 49\n",
      "7 https://news.yahoo.co.jp/zasshi (56850) 94\n",
      "180 https://news.yahoo.co.jp/polls/domestic/30363/result (26728) 49\n",
      "160 https://news.yahoo.co.jp/pickup/6254189 (41959) 56\n",
      "97 https://news.yahoo.co.jp/polls/sports/30443/result (25921) 49\n",
      "117 https://news.yahoo.co.jp/ranking/access?ty=p (51858) 58\n",
      "235 https://news.yahoo.co.jp/zasshi?c=env (60876) 101\n",
      "182 https://news.yahoo.co.jp/polls/domestic/30283/result (26232) 49\n",
      "35 https://news.yahoo.co.jp/ranking/access?ty=b (51204) 97\n",
      "335 https://news.yahoo.co.jp/byline/shinmukoeng/20170914-00075629 (45193) 62\n",
      "112 https://news.yahoo.co.jp/polls/sports/28382/result (28226) 49\n",
      "29 https://news.yahoo.co.jp/topics (43941) 133\n",
      "90 https://news.yahoo.co.jp/polls/easy/list?sort=new (32775) 82\n",
      "242 https://news.yahoo.co.jp/zasshi?c=bus_all (63061) 109\n",
      "384 https://news.yahoo.co.jp/pickup/6253999 (41279) 56\n",
      "225 https://news.yahoo.co.jp/zasshi?c=spo (63823) 112\n",
      "113 https://news.yahoo.co.jp/polls/sports/28262/result (27382) 49\n",
      "43 https://news.yahoo.co.jp/polls/domestic/30484/vote (28255) 50\n",
      "177 https://news.yahoo.co.jp/polls/domestic/30423/result (26524) 49\n",
      "291 https://news.yahoo.co.jp/zasshi?c=asent&p=1 (62399) 103\n",
      "136 https://news.yahoo.co.jp/ranking/access?ty=t&c=bus (43151) 57\n",
      "311 https://news.yahoo.co.jp/media/list?m=nipponcom (39097) 42\n",
      "186 https://news.yahoo.co.jp/polls/domestic/list?page=3 (33527) 87\n",
      "371 https://news.yahoo.co.jp/pickup/6254171 (42216) 56\n",
      "132 https://news.yahoo.co.jp/pickup/economy/rss.xml Ignore non text/html page\n",
      "249 https://news.yahoo.co.jp/zasshi?c=kr (61038) 110\n",
      "162 https://news.yahoo.co.jp/pickup/6254154 (41722) 57\n",
      "350 https://news.yahoo.co.jp/byline/shinmukoeng/20170911-00075479 (45044) 62\n",
      "292 https://news.yahoo.co.jp/zasshi?c=c_spo&p=1 (65677) 112\n",
      "365 https://news.yahoo.co.jp/pickup/6254178 (41160) 56\n",
      "374 https://news.yahoo.co.jp/pickup/6254138 (41800) 56\n",
      "149 https://news.yahoo.co.jp/hl?c=dom&p=8 (39926) 62\n",
      "241 https://news.yahoo.co.jp/zasshi?c=bus (62935) 109\n",
      "505 https://news.yahoo.co.jp/zasshi?c=bus&p=3 (62826) 110\n",
      "441 https://news.yahoo.co.jp/zasshi?c=spo&p=5 (64946) 113\n",
      "440 https://news.yahoo.co.jp/zasshi?c=spo&p=4 (64973) 113\n",
      "81 https://news.yahoo.co.jp/pickup/domestic/rss.xml Ignore non text/html page\n",
      "46 https://news.yahoo.co.jp/ranking/comment/rate (96110) 57\n",
      "213 https://news.yahoo.co.jp/media?ty=z&disp=c (99182) 249\n",
      "534 https://news.yahoo.co.jp/media/list?m=globisv (54241) 41\n",
      "309 https://news.yahoo.co.jp/media/list?m=alterna (54070) 51\n",
      "520 https://news.yahoo.co.jp/media/list?m=will (44644) 49\n",
      "329 https://news.yahoo.co.jp/byline/sugieyuji/20170914-00075746 (44160) 62\n",
      "403 https://news.yahoo.co.jp/polls/easy/list?sort=new&page=2 (32872) 84\n",
      "615 https://news.yahoo.co.jp/media/list?m=will&p=8 (45219) 50\n",
      "170 https://news.yahoo.co.jp/polls/list?category=1&sort=new&state=open (26548) 63\n",
      "496 https://news.yahoo.co.jp/zasshi?c=c_spo&p=7 (65627) 113\n",
      "65 https://news.yahoo.co.jp/polls/domestic/30763/result (25172) 50\n",
      "223 https://news.yahoo.co.jp/zasshi?c=asent (62399) 103\n",
      "38 https://news.yahoo.co.jp/ranking/access?ty=t&c=dom (42484) 57\n",
      "135 https://news.yahoo.co.jp/ted (33886) 60\n",
      "465 https://news.yahoo.co.jp/polls/domestic/29363/result (26089) 49\n",
      "121 https://news.yahoo.co.jp/hl?c=biz (39339) 66\n",
      "301 https://news.yahoo.co.jp/zasshi?c=sci&p=1 (61095) 108\n",
      "44 https://news.yahoo.co.jp/promo/app/yjnews (8519) 2\n",
      "502 https://news.yahoo.co.jp/hl?c=dom&p=11 (41274) 62\n",
      "560 https://news.yahoo.co.jp/media/list?m=gooday (43249) 39\n",
      "413 https://news.yahoo.co.jp/polls/easy/18123/result (26782) 49\n",
      "683 https://news.yahoo.co.jp/zasshi?c=sci&p=9 (62625) 109\n",
      "100 https://news.yahoo.co.jp/polls/sports/30203/result (36893) 49\n",
      "217 https://news.yahoo.co.jp/zasshi?c=peo (62500) 107\n",
      "206 https://news.yahoo.co.jp/polls/sports/26462/result (27273) 49\n",
      "151 https://news.yahoo.co.jp/hl?c=soci&p=2 (41185) 63\n",
      "28 https://news.yahoo.co.jp/pickup/6254184 (41389) 56\n",
      "569 https://news.yahoo.co.jp/media/list?m=jisin (54284) 49\n",
      "298 https://news.yahoo.co.jp/zasshi?c=golf&p=1 (65018) 112\n",
      "300 https://news.yahoo.co.jp/zasshi?c=c_sci&p=1 (61906) 108\n",
      "166 https://news.yahoo.co.jp/pickup/6254091 (41243) 56\n",
      "102 https://news.yahoo.co.jp/polls/sports/29483/result (26085) 50\n",
      "4 https://news.yahoo.co.jp/profile/settings (15880) 5\n",
      "700 https://news.yahoo.co.jp/hl?c=soci&p=1 (44034) 65\n",
      "438 https://news.yahoo.co.jp/zasshi?c=spo&p=2 (64898) 113\n",
      "597 https://news.yahoo.co.jp/media/list?m=globisv&p=2 (54249) 42\n",
      "515 https://news.yahoo.co.jp/media/list?m=gendaibiz (54245) 51\n",
      "530 https://news.yahoo.co.jp/media/list?m=bpnet (28087) 42\n",
      "699 https://news.yahoo.co.jp/ranking/comment/rate?ty=z&c=dom (52974) 56\n",
      "348 https://news.yahoo.co.jp/byline/shinmukoeng/20170913-00075523 (46493) 62\n",
      "638 https://news.yahoo.co.jp/polls/easy/10941/result (27733) 49\n",
      "22 https://news.yahoo.co.jp/pickup/6254194 (41719) 56\n",
      "650 https://news.yahoo.co.jp/polls/easy/9221/result (26533) 49\n",
      "504 https://news.yahoo.co.jp/zasshi?c=bus&p=2 (64092) 110\n",
      "501 https://news.yahoo.co.jp/hl?c=dom&p=10 (41378) 62\n",
      "583 https://news.yahoo.co.jp/media/list?m=vingtcinqw (56681) 41\n",
      "324 https://news.yahoo.co.jp/byline/mizushimahiroaki/20170915-00075765 (47676) 62\n",
      "402 https://news.yahoo.co.jp/polls/list?category=5&sort=new&state=close (32958) 82\n",
      "339 https://news.yahoo.co.jp/byline/international (35171) 65\n",
      "201 https://news.yahoo.co.jp/polls/sports/27422/result (26130) 49\n",
      "508 https://news.yahoo.co.jp/zasshi?c=bus&p=6 (64305) 110\n",
      "How many pages:\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages\n",
    "    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT,\n",
    "     error INTEGER, old_rank REAL, new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links\n",
    "    (from_id INTEGER, to_id INTEGER)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')\n",
    "\n",
    "# Check to see if we are already in progress...\n",
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()\n",
    "if row is not None:\n",
    "    print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")\n",
    "else :\n",
    "    starturl = input('Enter web url or enter: ')\n",
    "    if ( len(starturl) < 1 ) : starturl = 'http://www.dr-chuck.com/'\n",
    "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "    web = starturl\n",
    "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    if ( len(web) > 1 ) :\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) )\n",
    "        conn.commit()\n",
    "\n",
    "# Get the current webs\n",
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print(webs)\n",
    "\n",
    "many = 0\n",
    "while True:\n",
    "    if ( many < 1 ) :\n",
    "        sval = input('How many pages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        many = int(sval)\n",
    "    many = many - 1\n",
    "\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        # print row\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print('No unretrieved HTML pages found')\n",
    "        many = 0\n",
    "        break\n",
    "\n",
    "    print(fromid, url, end=' ')\n",
    "\n",
    "    # If we are retrieving this page, there should be no links from it\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "    try:\n",
    "        document = urlopen(url, context=ctx)\n",
    "\n",
    "        html = document.read()\n",
    "        if document.getcode() != 200 :\n",
    "            print(\"Error on page: \",document.getcode())\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "        if 'text/html' != document.info().get_content_type() :\n",
    "            print(\"Ignore non text/html page\")\n",
    "            cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n",
    "            cur.execute('UPDATE Pages SET error=0 WHERE url=?', (url, ) )\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print('('+str(len(html))+')', end=' ')\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except:\n",
    "        print(\"Unable to retrieve or parse page\")\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        href = tag.get('href', None)\n",
    "        if ( href is None ) : continue\n",
    "        # Resolve relative references like href=\"/contact\"\n",
    "        up = urlparse(href)\n",
    "        if ( len(up.scheme) < 1 ) :\n",
    "            href = urljoin(url, href)\n",
    "        ipos = href.find('#')\n",
    "        if ( ipos > 1 ) : href = href[:ipos]\n",
    "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "        if ( href.endswith('/') ) : href = href[:-1]\n",
    "        # print href\n",
    "        if ( len(href) < 1 ) : continue\n",
    "\n",
    "\t\t# Check if the URL is in any of the webs\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if ( href.startswith(web) ) :\n",
    "                found = True\n",
    "                break\n",
    "        if not found : continue\n",
    "\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print('Could not retrieve id')\n",
    "            continue\n",
    "        # print fromid, toid\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )\n",
    "\n",
    "\n",
    "    print(count)\n",
    "\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, None, 1.0, 1, 'https://news.yahoo.co.jp')\n",
      "(246, None, 1.0, 7, 'https://news.yahoo.co.jp/zasshi')\n",
      "(142, None, 1.0, 12, 'https://news.yahoo.co.jp/hl?c=dom')\n",
      "(136, None, 1.0, 14, 'https://news.yahoo.co.jp/hl?c=bus')\n",
      "(111, None, 1.0, 4, 'https://news.yahoo.co.jp/profile/settings')\n",
      "(99, None, 1.0, 44, 'https://news.yahoo.co.jp/promo/app/yjnews')\n",
      "(56, None, 1.0, 213, 'https://news.yahoo.co.jp/media?ty=z&disp=c')\n",
      "(48, None, 1.0, 39, 'https://news.yahoo.co.jp/polls/domestic/30763/vote')\n",
      "(47, None, 1.0, 42, 'https://news.yahoo.co.jp/polls/domestic/30683/vote')\n",
      "(47, None, 1.0, 43, 'https://news.yahoo.co.jp/polls/domestic/30484/vote')\n",
      "(34, None, 1.0, 59, 'https://news.yahoo.co.jp/polls/domestic/list')\n",
      "(34, None, 1.0, 61, 'https://news.yahoo.co.jp/polls/sports/list')\n",
      "(34, None, 1.0, 66, 'https://news.yahoo.co.jp/polls/entertainment/30783/vote')\n",
      "(33, None, 1.0, 21, 'https://news.yahoo.co.jp/pickup/6254197')\n",
      "(29, None, 1.0, 241, 'https://news.yahoo.co.jp/zasshi?c=bus')\n",
      "(23, None, 1.0, 28, 'https://news.yahoo.co.jp/pickup/6254184')\n",
      "(22, None, 1.0, 22, 'https://news.yahoo.co.jp/pickup/6254194')\n",
      "(22, None, 1.0, 291, 'https://news.yahoo.co.jp/zasshi?c=asent&p=1')\n",
      "(22, None, 1.0, 292, 'https://news.yahoo.co.jp/zasshi?c=c_spo&p=1')\n",
      "(22, None, 1.0, 298, 'https://news.yahoo.co.jp/zasshi?c=golf&p=1')\n",
      "(22, None, 1.0, 300, 'https://news.yahoo.co.jp/zasshi?c=c_sci&p=1')\n",
      "(22, None, 1.0, 301, 'https://news.yahoo.co.jp/zasshi?c=sci&p=1')\n",
      "(17, None, 1.0, 46, 'https://news.yahoo.co.jp/ranking/comment/rate')\n",
      "(16, None, 1.0, 79, 'https://news.yahoo.co.jp/pickup/6254198')\n",
      "(15, None, 1.0, 38, 'https://news.yahoo.co.jp/ranking/access?ty=t&c=dom')\n",
      "(13, None, 1.0, 36, 'https://news.yahoo.co.jp/ranking/comment/rate?ty=t')\n",
      "(13, None, 1.0, 37, 'https://news.yahoo.co.jp/ranking/access?ty=v')\n",
      "(13, None, 1.0, 242, 'https://news.yahoo.co.jp/zasshi?c=bus_all')\n",
      "(11, None, 1.0, 35, 'https://news.yahoo.co.jp/ranking/access?ty=b')\n",
      "(10, None, 1.0, 117, 'https://news.yahoo.co.jp/ranking/access?ty=p')\n",
      "(10, None, 1.0, 137, 'https://news.yahoo.co.jp/ranking/access?ty=z&c=bus')\n",
      "(10, None, 1.0, 225, 'https://news.yahoo.co.jp/zasshi?c=spo')\n",
      "(8, None, 1.0, 72, 'https://news.yahoo.co.jp/hl?c=soci')\n",
      "(7, None, 1.0, 90, 'https://news.yahoo.co.jp/polls/easy/list?sort=new')\n",
      "(7, None, 1.0, 130, 'https://news.yahoo.co.jp/pickup/6254155')\n",
      "(7, None, 1.0, 136, 'https://news.yahoo.co.jp/ranking/access?ty=t&c=bus')\n",
      "(7, None, 1.0, 235, 'https://news.yahoo.co.jp/zasshi?c=env')\n",
      "(7, None, 1.0, 335, 'https://news.yahoo.co.jp/byline/shinmukoeng/20170914-00075629')\n",
      "(7, None, 1.0, 504, 'https://news.yahoo.co.jp/zasshi?c=bus&p=2')\n",
      "(6, None, 1.0, 84, 'https://news.yahoo.co.jp/ranking/access?ty=z&c=dom')\n",
      "(6, None, 1.0, 94, 'https://news.yahoo.co.jp/polls/sports/list?page=2')\n",
      "(6, None, 1.0, 121, 'https://news.yahoo.co.jp/hl?c=biz')\n",
      "(6, None, 1.0, 151, 'https://news.yahoo.co.jp/hl?c=soci&p=2')\n",
      "(6, None, 1.0, 309, 'https://news.yahoo.co.jp/media/list?m=alterna')\n",
      "(6, None, 1.0, 311, 'https://news.yahoo.co.jp/media/list?m=nipponcom')\n",
      "(6, None, 1.0, 339, 'https://news.yahoo.co.jp/byline/international')\n",
      "(6, None, 1.0, 403, 'https://news.yahoo.co.jp/polls/easy/list?sort=new&page=2')\n",
      "(6, None, 1.0, 438, 'https://news.yahoo.co.jp/zasshi?c=spo&p=2')\n",
      "(6, None, 1.0, 440, 'https://news.yahoo.co.jp/zasshi?c=spo&p=4')\n",
      "(6, None, 1.0, 441, 'https://news.yahoo.co.jp/zasshi?c=spo&p=5')\n",
      "108 rows.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "     FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "     WHERE html IS NOT NULL\n",
    "     GROUP BY id ORDER BY inbound DESC''')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    if count < 50 : print(row)\n",
    "    count = count + 1\n",
    "print(count, 'rows.')\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations:30\n",
      "1 5.3410423061918965e-05\n",
      "2 2.6695173789077714e-05\n",
      "3 1.3595939096160523e-05\n",
      "4 7.007842802021948e-06\n",
      "5 3.641853323776181e-06\n",
      "6 1.9344787301257467e-06\n",
      "7 1.0246419063853598e-06\n",
      "8 5.399522876424068e-07\n",
      "9 2.830726464577499e-07\n",
      "10 1.4784035277320356e-07\n",
      "11 7.701049756881104e-08\n",
      "12 4.003370335172873e-08\n",
      "13 2.077879042888911e-08\n",
      "14 1.0772462569094596e-08\n",
      "15 5.580001576411726e-09\n",
      "16 2.8884478591101533e-09\n",
      "17 1.4944309282858857e-09\n",
      "18 7.728979420208572e-10\n",
      "19 3.996160831512655e-10\n",
      "20 2.0656980010270022e-10\n",
      "21 1.0676236271074308e-10\n",
      "22 5.5171672143412895e-11\n",
      "23 2.850845280009282e-11\n",
      "24 1.4729601669030804e-11\n",
      "25 7.610211329424612e-12\n",
      "26 3.931651043826074e-12\n",
      "27 2.031148140626384e-12\n",
      "28 1.0491873573640713e-12\n",
      "29 5.41871974245888e-13\n",
      "30 2.7965361507990377e-13\n",
      "[(1, 13.477613245254474), (21, 3.881429389852794), (36, 1.8012192948183097), (39, 1.975696824665033), (12, 7.37330406790641)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Find the ids that send out page rank - we only are interested\n",
    "# in pages in the SCC that have in and out links\n",
    "cur.execute('''SELECT DISTINCT from_id FROM Links''')\n",
    "from_ids = list()\n",
    "for row in cur: \n",
    "    from_ids.append(row[0])\n",
    "\n",
    "# Find the ids that receive page rank \n",
    "to_ids = list()\n",
    "links = list()\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "for row in cur:\n",
    "    from_id = row[0]\n",
    "    to_id = row[1]\n",
    "    if from_id == to_id : continue\n",
    "    if from_id not in from_ids : continue\n",
    "    if to_id not in from_ids : continue\n",
    "    links.append(row)\n",
    "    if to_id not in to_ids : to_ids.append(to_id)\n",
    "\n",
    "# Get latest page ranks for strongly connected component\n",
    "prev_ranks = dict()\n",
    "for node in from_ids:\n",
    "    cur.execute('''SELECT new_rank FROM Pages WHERE id = ?''', (node, ))\n",
    "    row = cur.fetchone()\n",
    "    prev_ranks[node] = row[0]\n",
    "\n",
    "sval = input('How many iterations:')\n",
    "many = 1\n",
    "if ( len(sval) > 0 ) : many = int(sval)\n",
    "\n",
    "# Sanity check\n",
    "if len(prev_ranks) < 1 : \n",
    "    print(\"Nothing to page rank.  Check data.\")\n",
    "    quit()\n",
    "\n",
    "# Lets do Page Rank in memory so it is really fast\n",
    "for i in range(many):\n",
    "    # print prev_ranks.items()[:5]\n",
    "    next_ranks = dict();\n",
    "    total = 0.0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        total = total + old_rank\n",
    "        next_ranks[node] = 0.0\n",
    "    # print total\n",
    "\n",
    "    # Find the number of outbound links and sent the page rank down each\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        # print node, old_rank\n",
    "        give_ids = list()\n",
    "        for (from_id, to_id) in links:\n",
    "            if from_id != node : continue\n",
    "           #  print '   ',from_id,to_id\n",
    "\n",
    "            if to_id not in to_ids: continue\n",
    "            give_ids.append(to_id)\n",
    "        if ( len(give_ids) < 1 ) : continue\n",
    "        amount = old_rank / len(give_ids)\n",
    "        # print node, old_rank,amount, give_ids\n",
    "    \n",
    "        for id in give_ids:\n",
    "            next_ranks[id] = next_ranks[id] + amount\n",
    "    \n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "    evap = (total - newtot) / len(next_ranks)\n",
    "\n",
    "    # print newtot, evap\n",
    "    for node in next_ranks:\n",
    "        next_ranks[node] = next_ranks[node] + evap\n",
    "\n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "\n",
    "    # Compute the per-page average change from old rank to new rank\n",
    "    # As indication of convergence of the algorithm\n",
    "    totdiff = 0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        new_rank = next_ranks[node]\n",
    "        diff = abs(old_rank-new_rank)\n",
    "        totdiff = totdiff + diff\n",
    "\n",
    "    avediff = totdiff / len(prev_ranks)\n",
    "    print(i+1, avediff)\n",
    "\n",
    "    # rotate\n",
    "    prev_ranks = next_ranks\n",
    "\n",
    "# Put the final ranks back into the database\n",
    "print(list(next_ranks.items())[:5])\n",
    "cur.execute('''UPDATE Pages SET old_rank=new_rank''')\n",
    "for (id, new_rank) in list(next_ranks.items()) :\n",
    "    cur.execute('''UPDATE Pages SET new_rank=? WHERE id=?''', (new_rank, id))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating JSON output on spider.js...\n",
      "How many nodes? 30\n",
      "Open force.html in a browser to view the visualization\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Creating JSON output on spider.js...\")\n",
    "howmany = int(input(\"How many nodes? \"))\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "    FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "    WHERE html IS NOT NULL AND ERROR IS NULL\n",
    "    GROUP BY id ORDER BY id,inbound''')\n",
    "\n",
    "fhand = open('spider.js','w')\n",
    "nodes = list()\n",
    "maxrank = None\n",
    "minrank = None\n",
    "for row in cur :\n",
    "    nodes.append(row)\n",
    "    rank = row[2]\n",
    "    if maxrank is None or maxrank < rank: maxrank = rank\n",
    "    if minrank is None or minrank > rank : minrank = rank\n",
    "    if len(nodes) > howmany : break\n",
    "\n",
    "if maxrank == minrank or maxrank is None or minrank is None:\n",
    "    print(\"Error - please run sprank.py to compute page rank\")\n",
    "    quit()\n",
    "\n",
    "fhand.write('spiderJson = {\"nodes\":[\\n')\n",
    "count = 0\n",
    "map = dict()\n",
    "ranks = dict()\n",
    "for row in nodes :\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    # print row\n",
    "    rank = row[2]\n",
    "    rank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{'+'\"weight\":'+str(row[0])+',\"rank\":'+str(rank)+',')\n",
    "    fhand.write(' \"id\":'+str(row[3])+', \"url\":\"'+row[4]+'\"}')\n",
    "    map[row[3]] = count\n",
    "    ranks[row[3]] = rank\n",
    "    count = count + 1\n",
    "fhand.write('],\\n')\n",
    "\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "fhand.write('\"links\":[\\n')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    # print row\n",
    "    if row[0] not in map or row[1] not in map : continue\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    rank = ranks[row[0]]\n",
    "    srank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{\"source\":'+str(map[row[0]])+',\"target\":'+str(map[row[1]])+',\"value\":3}')\n",
    "    count = count + 1\n",
    "fhand.write(']};')\n",
    "fhand.close()\n",
    "cur.close()\n",
    "\n",
    "print(\"Open force.html in a browser to view the visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
